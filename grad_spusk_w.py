'''
ТУТ будет рассматриваться градиентный спуск для расчета вектора весов w для винимизации функции потерь.
Принцип работы алгоритма сохраняется,за исключением того, что теперь мы рассматриваем n-мерный вектор w = [w1, w2,...,wn]
В таком случае в каждой точки будем определять не одну производную, а n производных
grad(Q(wt)) = [dQ(wt)/dw1^t ,..., dQ(wt)/dwn^t] - данный вектор будет определять направлеие вектора в n- мерном пространстве
А далее движемся в сторону антиградиента, для приближения к минимуму w(t+1) = w(t) - La * grad(Q(wt))
В качестве grad(Q(wt)) - будет сумма градиентов от функции потерь по всем объектам обучающей выборки:
w(t+1) = w(t) - La * sum(grad(Li(wt, xi))), но во многих задачах ML обучающая выборка содержит очень много объектов, по этому
считать каждый раз градиент для каждого объекта это очень затратно. по этому sum(grad(Li(wt, xi))) заменяют на псевдоградиент
(1он должен образовывать острый угол с реальным градиентом
2он должен вычисляться проще чем sum(grad(Li(wt, xi)))
)
для этього будем вычислять градиент не по каждому объекту, а только по случайному k объекту:
w(t+1) = w(t) - La * sum(grad(Lk(wt, xk))) - данный градиент в среднем будет образовывать острый угол с истинным градиентом
Данный алгоритм называют стахостическим градиентным спуском.

Для контроля качества нам также нужно будет вычислять функцию потерь Q(w) =1/l *sum(Li(w))
и в случае с большой выборкой,это будет также затратно
и в результате не получится сократить вычисления, для решения данной проблемы используют
Q = G * epk + (1 - G) * Q - экспониницальное скользящее среднее
epk - ошибки по функциям потерь для k объекта
G - скорость "забывания"
Обычное среднее : Q = 1/m * (epm + epm-1 +...ep1 )
m*Q - epm = epm-1 +...ep1
Qm-1 = 1/m-1 * [m*Qm - epm] = m/m-1 *Qm - 1/ m-1 epm ->
Qm = m-1/m *  Qm-1 + m-1/m * 1/m-1 *epm
Qm = 1/m * epm + (1- 1/m) Qm-1  для  экспониницального скользящего среднего  вместо 1/m используем G (Qm-1 - среднее по m-1 слагаемому) ->
Qm = G * epm + (1- G) Qm-1, если расписать ее по слагаемым:
Qm = G * epm + (1- G) G* epm-1 + (1- G)^2 G* epm-2 +...
G лучше определять по такой формуле:
G = 2/N+1 при N = 99 G = 0.02 (это значит что мы будем учитывать 99 мслагаемых)
данным способом мы будем порлучать примерную оценку качества.
Но у такого подхода есть большой минус - неравномерная сходимость к точке минимума, если бы мы проводили градиентный спуск, то сходились бы по гладкой прямой,
в случае же стахостического градиентного спуска, появляются "блуждания" , для избегания этого, стиоит сделать антиградиент не по одному k образу, а
по группе образов (по батчам), так делаем функцию сходимости плавнее.
'''

import numpy as np
import matplotlib.pyplot as plt


# сигмоидная функция потерь
def loss(w, x, y):
    M = np.dot(w, x) * y
    return 2 / (1 + np.exp(M))


# производная сигмоидной функции потерь по вектору w
def df(w, x, y):
    M = np.dot(w, x) * y
    return -2 * (1 + np.exp(M)) ** (-2) * np.exp(M) * x * y


# обучающая выборка с тремя признаками (третий - константа +1)
x_train = [[10, 50], [20, 30], [25, 30], [20, 60], [15, 70], [40, 40], [30, 45], [20, 45], [40, 30], [7, 35]]
x_train = [x + [1] for x in x_train]
x_train = np.array(x_train)
y_train = np.array([-1, 1, 1, -1, -1, 1, 1, -1, 1, -1])

n_train = len(x_train)  # размер обучающей выборки
w = [0.0, 0.0, 0.0]  # начальные весовые коэффициенты
nt = 0.0005  # шаг сходимости SGD
lm = 0.01  # скорость "забывания" для Q
N = 500  # число итераций SGD

Q = np.mean([loss(w, x, y) for x, y in zip(x_train, y_train)])  # показатель качества
Q_plot = [Q]

for i in range(N):
    k = np.random.randint(0, n_train - 1)  # случайный индекс
    ek = loss(w, x_train[k], y_train[k])  # вычисление потерь для выбранного вектора
    w = w - nt * df(w, x_train[k], y_train[k])  # корректировка весов по SGD
    Q = lm * ek + (1 - lm) * Q  # пересчет показателя качества
    Q_plot.append(Q)

print(w)
print(Q_plot)

line_x = list(range(max(x_train[:, 0])))  # формирование графика разделяющей линии
line_y = [-x * w[0] / w[1] - w[2] / w[1] for x in line_x]

x_0 = x_train[y_train == 1]  # формирование точек для 1-го
x_1 = x_train[y_train == -1]  # и 2-го классов

plt.scatter(x_0[:, 0], x_0[:, 1], color='red')
plt.scatter(x_1[:, 0], x_1[:, 1], color='blue')
plt.plot(line_x, line_y, color='green')

plt.xlim([0, 45])
plt.ylim([0, 75])
plt.ylabel("длина")
plt.xlabel("ширина")
plt.grid(True)
plt.show()

