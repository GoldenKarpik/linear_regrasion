'''
ранее в качестве функции потерь использовалась пороговая функция отступ, но
он недифференцируем (в точке 0 нет производной, а в других частях будет константа).
Возьмем похожую функцию, но при этом дифферуемую. (можно взять любую дифферуемую функцию потерь, в зависимости от
функции будет разный метод, хотя суть одна, раз6ница лишь в функции потерь
нгапример, если возьмем V(M) = (1-M) - кусочно - линейная (SVM), то получим метод опорных векторов,
если взять L(M) = log2(1+e^(-M)) - логарифмическая (LR) - то логистичсекая регрессия
также есть:
H(M) = (-M) - кусочно -линейная (Hebbs rule)
Q(M) = (1-M)^2 - квадратичная (FLD)
S(M) = 2*(1+e^M)^-1 - сигмоида (ANN)
E(M) = e^-M - экспоненциальная (AdaBoost)
Все они по  значению больше исходной пароговой функции и убывают при увеличении отступа =>
инимизируя их, мы будем минимизировать нашу изначальную функцию sum[Mi<0] -> min , но с некоторой погрешностью, т.к.
sum[Mi<0] <= O(L)
тут рассмотрим туже задачу, но уже с использованием Q(M) = (1-M)^2 - квадратичная
Q(M) = (1-M)^2 = (1 - w^T * xi * yi)^2
Q(w) = sum(1 - w^T * xi * yi)^2 -> min
для минимизации данной функции остается только посчитать производную данной функции по w и приравнять к 0:
-2 sum(1 - w^T * xi * yi)*xi^T * yi =0
sum(xi^T * yi - w^T) * sum(si *xi^T * yi^2) = 0
выразим w:
w^T = sum (xi^T * yi) * (sum xi * xi^T)^-1)

в нашем случае w = [w0, w1, w2] все как и в прошлой задачи, но тут w0 внесена в вектор. ->
w2x2 + w1x1 + w0 = 0, x = [x1 , x2 , 1]
)
'''

import numpy as np
import matplotlib.pyplot as plt

x_train = [[10, 50], [20, 30], [25, 30], [20, 60], [15, 70], [40, 40], [30, 45], [20, 45], [40, 30], [7, 35]]
x_train = [x + [1] for x in x_train]
x_train = np.array(x_train)
y_train = np.array([-1, 1, 1, -1, -1, 1, 1, -1, 1, -1])

#w^T = sum (xi^T * yi) * (sum xi * xi^T)^-1)

pt = np.sum([x * y for x, y in zip(x_train, y_train)], axis=0) #  sum (xi^T * yi)
xxt = np.sum([np.outer(x, x) for x in x_train], axis=0) #(sum xi * xi^T)^-1
w = np.dot(pt, np.linalg.inv(xxt)) # перемножаем результаты, с чуетом что должна быть обратная матрица.
print(w)

line_x = list(range(max(x_train[:, 0])))    # формирование графика разделяющей линии
line_y = [-x*w[0]/w[1] - w[2]/w[1] for x in line_x]

x_0 = x_train[y_train == 1]                 # формирование точек для 1-го
x_1 = x_train[y_train == -1]                # и 2-го классов

plt.scatter(x_0[:, 0], x_0[:, 1], color='red')
plt.scatter(x_1[:, 0], x_1[:, 1], color='blue')
plt.plot(line_x, line_y, color='green')

plt.xlim([0, 45])
plt.ylim([0, 75])
plt.ylabel("длина")
plt.xlabel("ширина")
plt.grid(True)
plt.show()

