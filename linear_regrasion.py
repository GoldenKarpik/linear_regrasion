import numpy as np
import matplotlib.pyplot as plt

# тут как обучающие параметры будем использовать параметры классов (длинну и ширину)
X_train = np.array([[10,50],[20,30],[25,30],[20,60],[15,70],[40,40],[30,45],[20,45],[40,30],[7,35]])
# в качестве классов используем бинарные значения 1 и -1 (для вычисленгия "отступа") в этой задачи это были божьи коровки и гусеницы
Y_train = np.array([-1,1,1,-1,-1,1,1,-1,1,-1])

n_train = len(X_train)  #размер обучающей выборки, что логично
# уравнение гиперплоскости (немного пояснений, что бы не забыть) - бинарная классификация.
'''
по факту нам необходимо разделить классы в пространстве с помощью гиперплоскости (тут речь пока пойдет только о прямой, без учета полиномов). 
В данной задачи будем использовать разделяющую прямую a(x) = kx+b. Пусть по оси x1 у нас лежат параметры длинны, а по оси x2 - ширна,
в таком случае наша прямая будет выглядить как x2 = a(x1) => x2 = kx1+b => x2-kx1-b=0 получим такое уравнение и запишем его в виде :
wx2+wx1-w0 = 0 теперь можно переписать его на уровень векторов (w = [w1, w2] , x = [x1, x2]): <w, x> -w0 = 0 
(<w, x> скалярное произведение векторов, <w, x>  = w^T * x = w1x1 + w2x2) w0 - смещение прямой по оси ординат. 
Вектор w всегда ортагонален вектору x. 
Например пусть w = [-1, 1], в таком случае  w1x1 + w2x2 = -x1 + x2 = 0 => x1=x2 - то есть прямая проходит через начало кординат под углом 45 градусов. 
Тоесть в таком случае вектор x и w составляют угол в 90 градусов. <w, x> = w1x1 + w2x2 = |w|*|x|*cos(w,x) = 0, 
В этом как раз и есть основная суть данного подхода. (тут стоило бы добавит график с рисунком, но в коде не получится)
Если наша линия действительно делит образы на 2 класса, то для любой точки класса 1, произведение ее радиус вектора с вектором w,
будет положительным, т.к. данные вектора образуют острый угол => |w|*|r|*cos(w,r) > 0 (т.к. косинус острого угла  больше 0)
и обратная ситуация для объектов класса -1, их радиус вектор будет образовывать с w тупой угол, где косинус меньше 0, в таком случае 
|w|*|r|*cos(w,r) < 0, так мы разделяем прямой два класса. 

w0 можно внести в вектор w, тогда получим 3х мерное пространство, а вместо линии будет плоскость, но результат по факту не изменится: w = [w0, w1, w2] и тогда x = [-1 , x1, x2]
<w, x> = -1*w0 + w1x1 + w2x2

'''
# собственно вектор w и будет задавать нашу гиперплоскость, тоесть основной задачей будет найти данный вектор.

'''
По графику видно, что выборка представляет собой линейно разделимые классы, и прямую можно провести через начало кординат =>
решение будем искать в виде w1x1+w2x2 = 0 , выразим x2 = -w1*x1/w2, в таком случе можем w2 = -1 и нам останется найти w1.
Для этого мы будем использовать "нотацию Айверсона" - число не верных классификаций sum(a(xi) != yi) 
(тоесть когда у нас будет неверная классификацияб функция вернет значение True и считаем их количество)
В нашем случае, где значения классов принимают значения 1 и -1, можно переписать функцию как: sum(yi*a(xi)<0) в таком случае значение -1 бует выпадать только когда классификация была не верной
Mi = a(xi)*yi - "отступ", показывае на сколько далеко находится объект от гипер плоскости и верно ли классифицируется объект.  
чем больше a(x) =  <w, x> тем объект дальше, если Mi >0 класс определн верно, если <0 , то не верно. 
критерий качества в нашем случе должен быть: sum(Mi<0) -> min (в идеале к 0) минимизируем ошибочные классификации.
sum(Mi<0) - кусочно непрерывная, недифференцируемая функиция, следовательно математически минимум не получится найти,
в таком случае будем испрользовать ML: 
В цикле по очередно перебераем наши объекты и их параметры (x и y)
далее смотрим если Mi = sign(<w, xi>)* yi <0, то 
корректируем веса w1 = w1 + L* yi  - в таком случае мы меняем w1 так что бы угловой коэфициент прямой изменился 
так что бы прямая проходила так что бы i-й образ классифицировался верно.
после чего вычисляем показатель качества sum (Mi< 0 )

'''

x_0 = X_train[Y_train==1]
x_1 = X_train[Y_train == -1]
plt.scatter(x_0[:,0], x_0[:,1], color = 'red')
plt.scatter(x_1[:,0], x_1[:,1], color = 'blue')

plt.xlim([0,45])
plt.ylim([0,75])
plt.ylabel('длинна')
plt.xlabel('ширина')
plt.grid(True)
plt.show()


w = [0, -1] # начальное значение вектора w
a = lambda x: np.sign(x[0]*w[0]+x[1]*w[1])
N = 50 # число итераций
L = 0.1 #шаг изменения веса
e = 0.1 # небольшая добавка для w0 что бы был зазор между разделяющей линией и образом
last_error_index = -1  # индекс последнего ошибочного наблюдения

for n in range(N):
    for i in range(n_train):
        if Y_train[i] * a(X_train[i])<0: # проверяем на ошибку классификации
            w[0] = w[0] + L * Y_train[i] # корректируем вес
            last_error_index = i

        Q =sum([1 for i in range(n_train) if Y_train[i] * a(X_train[i])<0])
        if Q == 0: # если все верно классифицированно останавливаем алгоритм
            break

if last_error_index > -1:
    w[0]=w[0] + e * Y_train[last_error_index]

print(w)
# график прямой
line_x = list(range(max(X_train[:,0])))
line_y = [w[0] * x for x in  line_x]


plt.scatter(x_0[:,0], x_0[:,1], color = 'red')
plt.scatter(x_1[:,0], x_1[:,1], color = 'blue')
plt.plot(line_x, line_y, color = 'green')

plt.xlim([0,45])
plt.ylim([0,75])
plt.ylabel('длинна')
plt.xlabel('ширина')
plt.grid(True)
plt.show()